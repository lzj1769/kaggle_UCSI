Train on 1109 mini-batches, validate on 278 mini-batches
Epoch: 1/200 |  time : 09/05/19-22:06:23
=================================================================
train_loss: 0.64338444, train_bce_loss: 0.42263029, train_dice_loss: 0.86413859, train_dice: 0.33060835
valid_loss: 0.59010782, valid_bce_loss: 0.35845607, valid_dice_loss: 0.82175957, valid_dice: 0.44062753
******** Validation loss improved from inf to 0.5901078195880642, saving state ********

Epoch: 2/200 |  time : 09/05/19-22:24:38
=================================================================
train_loss: 0.63416459, train_bce_loss: 0.41048185, train_dice_loss: 0.85784734, train_dice: 0.31796544
valid_loss: 0.58427944, valid_bce_loss: 0.34708202, valid_dice_loss: 0.82147685, valid_dice: 0.43312828
******** Validation loss improved from 0.5901078195880642 to 0.5842794375025111, saving state ********

Epoch: 3/200 |  time : 09/05/19-22:42:38
=================================================================
train_loss: 0.62965862, train_bce_loss: 0.40315118, train_dice_loss: 0.85616606, train_dice: 0.33111468
valid_loss: 0.58317624, valid_bce_loss: 0.34030351, valid_dice_loss: 0.82604897, valid_dice: 0.46795175
******** Validation loss improved from 0.5842794375025111 to 0.5831762412897974, saving state ********

Epoch: 4/200 |  time : 09/05/19-23:00:38
=================================================================
train_loss: 0.62627691, train_bce_loss: 0.39849164, train_dice_loss: 0.85406217, train_dice: 0.34970073
valid_loss: 0.57382181, valid_bce_loss: 0.33638423, valid_dice_loss: 0.81125939, valid_dice: 0.45614677
******** Validation loss improved from 0.5831762412897974 to 0.5738218083656091, saving state ********

Epoch: 5/200 |  time : 09/05/19-23:18:38
=================================================================
train_loss: 0.62374546, train_bce_loss: 0.39552429, train_dice_loss: 0.85196662, train_dice: 0.35402919
valid_loss: 0.57975650, valid_bce_loss: 0.33767705, valid_dice_loss: 0.82183595, valid_dice: 0.48177523

Epoch: 6/200 |  time : 09/05/19-23:36:37
=================================================================
train_loss: 0.62128623, train_bce_loss: 0.39218253, train_dice_loss: 0.85038993, train_dice: 0.36613366
valid_loss: 0.58560420, valid_bce_loss: 0.34456402, valid_dice_loss: 0.82664438, valid_dice: 0.47529404

Epoch: 7/200 |  time : 09/05/19-23:54:36
=================================================================
train_loss: 0.61799119, train_bce_loss: 0.38712572, train_dice_loss: 0.84885666, train_dice: 0.37954633
valid_loss: 0.57653911, valid_bce_loss: 0.33415183, valid_dice_loss: 0.81892639, valid_dice: 0.49860341

Epoch: 8/200 |  time : 09/06/19-00:12:37
=================================================================
train_loss: 0.61499899, train_bce_loss: 0.38261019, train_dice_loss: 0.84738779, train_dice: 0.39802844
valid_loss: 0.58394714, valid_bce_loss: 0.33882963, valid_dice_loss: 0.82906466, valid_dice: 0.50620515

Epoch: 9/200 |  time : 09/06/19-00:30:35
=================================================================
train_loss: 0.61194059, train_bce_loss: 0.37812626, train_dice_loss: 0.84575493, train_dice: 0.41096316
valid_loss: 0.57619519, valid_bce_loss: 0.33105570, valid_dice_loss: 0.82133468, valid_dice: 0.51157438

Epoch: 10/200 |  time : 09/06/19-00:48:34
=================================================================
train_loss: 0.60948171, train_bce_loss: 0.37392948, train_dice_loss: 0.84503394, train_dice: 0.42148149
valid_loss: 0.58227363, valid_bce_loss: 0.34631796, valid_dice_loss: 0.81822930, valid_dice: 0.48517561
Epoch     9: reducing learning rate of group 0 to 5.0000e-03.

Epoch: 11/200 |  time : 09/06/19-01:06:33
=================================================================
train_loss: 0.59968581, train_bce_loss: 0.36028315, train_dice_loss: 0.83908846, train_dice: 0.44726019
valid_loss: 0.57271705, valid_bce_loss: 0.32809789, valid_dice_loss: 0.81733621, valid_dice: 0.51404929
******** Validation loss improved from 0.5738218083656091 to 0.5727170508542507, saving state ********

Epoch: 12/200 |  time : 09/06/19-01:24:33
=================================================================
train_loss: 0.59602311, train_bce_loss: 0.35535679, train_dice_loss: 0.83668943, train_dice: 0.46064761
valid_loss: 0.57383549, valid_bce_loss: 0.33252070, valid_dice_loss: 0.81515028, valid_dice: 0.49709125

Epoch: 13/200 |  time : 09/06/19-01:42:33
=================================================================
train_loss: 0.59251532, train_bce_loss: 0.35055598, train_dice_loss: 0.83447465, train_dice: 0.46370387
valid_loss: 0.57447273, valid_bce_loss: 0.33103293, valid_dice_loss: 0.81791253, valid_dice: 0.50158826

Epoch: 14/200 |  time : 09/06/19-02:00:31
=================================================================
train_loss: 0.59002671, train_bce_loss: 0.34741901, train_dice_loss: 0.83263442, train_dice: 0.47007027
valid_loss: 0.57290908, valid_bce_loss: 0.33083844, valid_dice_loss: 0.81497971, valid_dice: 0.49690094

Epoch: 15/200 |  time : 09/06/19-02:18:30
=================================================================
train_loss: 0.58835225, train_bce_loss: 0.34512064, train_dice_loss: 0.83158385, train_dice: 0.47393612
valid_loss: 0.57782363, valid_bce_loss: 0.34314582, valid_dice_loss: 0.81250143, valid_dice: 0.47606746

Epoch: 16/200 |  time : 09/06/19-02:36:30
=================================================================
train_loss: 0.58587391, train_bce_loss: 0.34199286, train_dice_loss: 0.82975497, train_dice: 0.47741364
valid_loss: 0.58075927, valid_bce_loss: 0.33689659, valid_dice_loss: 0.82462194, valid_dice: 0.49775000

Epoch: 17/200 |  time : 09/06/19-02:54:29
=================================================================
train_loss: 0.58440345, train_bce_loss: 0.33966175, train_dice_loss: 0.82914514, train_dice: 0.48569903
valid_loss: 0.57467690, valid_bce_loss: 0.33481913, valid_dice_loss: 0.81453467, valid_dice: 0.49274665
Epoch    16: reducing learning rate of group 0 to 2.5000e-03.

Epoch: 18/200 |  time : 09/06/19-03:12:28
=================================================================
train_loss: 0.57693481, train_bce_loss: 0.32961922, train_dice_loss: 0.82425040, train_dice: 0.51036444
valid_loss: 0.57837443, valid_bce_loss: 0.33361640, valid_dice_loss: 0.82313247, valid_dice: 0.48495418

Epoch: 19/200 |  time : 09/06/19-03:30:26
=================================================================
train_loss: 0.57404960, train_bce_loss: 0.32574952, train_dice_loss: 0.82234968, train_dice: 0.51342497
valid_loss: 0.57927878, valid_bce_loss: 0.33483696, valid_dice_loss: 0.82372061, valid_dice: 0.45898063

Epoch: 20/200 |  time : 09/06/19-03:48:24
=================================================================
train_loss: 0.57110126, train_bce_loss: 0.32206818, train_dice_loss: 0.82013433, train_dice: 0.51616627
valid_loss: 0.57378874, valid_bce_loss: 0.33475818, valid_dice_loss: 0.81281930, valid_dice: 0.46090613

Epoch: 21/200 |  time : 09/06/19-04:06:21
=================================================================
train_loss: 0.56985506, train_bce_loss: 0.32077169, train_dice_loss: 0.81893843, train_dice: 0.52289639
valid_loss: 0.57440305, valid_bce_loss: 0.33691609, valid_dice_loss: 0.81189001, valid_dice: 0.46081438

Epoch: 22/200 |  time : 09/06/19-04:24:20
=================================================================
train_loss: 0.56626188, train_bce_loss: 0.31701424, train_dice_loss: 0.81550952, train_dice: 0.52015632
valid_loss: 0.57829972, valid_bce_loss: 0.33703927, valid_dice_loss: 0.81956017, valid_dice: 0.46300227

Epoch: 23/200 |  time : 09/06/19-04:42:19
=================================================================
train_loss: 0.56616460, train_bce_loss: 0.31681230, train_dice_loss: 0.81551690, train_dice: 0.52545632
valid_loss: 0.58313202, valid_bce_loss: 0.34419794, valid_dice_loss: 0.82206610, valid_dice: 0.44841790
Epoch    22: reducing learning rate of group 0 to 1.2500e-03.

Epoch: 24/200 |  time : 09/06/19-05:00:19
=================================================================
train_loss: 0.56155535, train_bce_loss: 0.31045882, train_dice_loss: 0.81265187, train_dice: 0.54498436
valid_loss: 0.57762459, valid_bce_loss: 0.33605551, valid_dice_loss: 0.81919366, valid_dice: 0.44354750

Epoch: 25/200 |  time : 09/06/19-05:18:17
=================================================================
train_loss: 0.55771602, train_bce_loss: 0.30644153, train_dice_loss: 0.80899051, train_dice: 0.54891386
valid_loss: 0.57701737, valid_bce_loss: 0.33394732, valid_dice_loss: 0.82008743, valid_dice: 0.44210921

Epoch: 26/200 |  time : 09/06/19-05:36:16
=================================================================
train_loss: 0.55712038, train_bce_loss: 0.30615195, train_dice_loss: 0.80808880, train_dice: 0.55029629
valid_loss: 0.57684885, valid_bce_loss: 0.33329129, valid_dice_loss: 0.82040641, valid_dice: 0.44452538

Epoch: 27/200 |  time : 09/06/19-05:54:15
=================================================================
train_loss: 0.55455484, train_bce_loss: 0.30346175, train_dice_loss: 0.80564794, train_dice: 0.55285482
valid_loss: 0.58017618, valid_bce_loss: 0.33613251, valid_dice_loss: 0.82421984, valid_dice: 0.44511115

Epoch: 28/200 |  time : 09/06/19-06:12:13
=================================================================
train_loss: 0.55369832, train_bce_loss: 0.30221459, train_dice_loss: 0.80518205, train_dice: 0.56199226
valid_loss: 0.58231002, valid_bce_loss: 0.34109149, valid_dice_loss: 0.82352855, valid_dice: 0.43468701

Epoch: 29/200 |  time : 09/06/19-06:30:13
=================================================================
train_loss: 0.55089789, train_bce_loss: 0.29968199, train_dice_loss: 0.80211379, train_dice: 0.56418447
valid_loss: 0.58589037, valid_bce_loss: 0.35194412, valid_dice_loss: 0.81983661, valid_dice: 0.42784006
Epoch    28: reducing learning rate of group 0 to 1.0000e-03.

Epoch: 30/200 |  time : 09/06/19-06:48:12
=================================================================
train_loss: 0.55027165, train_bce_loss: 0.29873081, train_dice_loss: 0.80181249, train_dice: 0.57089881
valid_loss: 0.58019747, valid_bce_loss: 0.34206903, valid_dice_loss: 0.81832592, valid_dice: 0.42610860

Epoch: 31/200 |  time : 09/06/19-07:06:10
=================================================================
train_loss: 0.54905649, train_bce_loss: 0.29731426, train_dice_loss: 0.80079872, train_dice: 0.57592958
valid_loss: 0.57779382, valid_bce_loss: 0.34351686, valid_dice_loss: 0.81207077, valid_dice: 0.42400830

Epoch: 32/200 |  time : 09/06/19-07:24:09
=================================================================
train_loss: 0.54779268, train_bce_loss: 0.29591080, train_dice_loss: 0.79967457, train_dice: 0.57791296
valid_loss: 0.58461266, valid_bce_loss: 0.34351447, valid_dice_loss: 0.82571085, valid_dice: 0.43644287

Epoch: 33/200 |  time : 09/06/19-07:42:09
=================================================================
train_loss: 0.54662512, train_bce_loss: 0.29486920, train_dice_loss: 0.79838103, train_dice: 0.57864745
valid_loss: 0.58170906, valid_bce_loss: 0.34048695, valid_dice_loss: 0.82293117, valid_dice: 0.42911825

Epoch: 34/200 |  time : 09/06/19-08:00:08
=================================================================
train_loss: 0.54585563, train_bce_loss: 0.29460902, train_dice_loss: 0.79710225, train_dice: 0.58035730
valid_loss: 0.58444589, valid_bce_loss: 0.34623427, valid_dice_loss: 0.82265751, valid_dice: 0.42310417

Epoch: 35/200 |  time : 09/06/19-08:18:08
=================================================================
train_loss: 0.54472072, train_bce_loss: 0.29297216, train_dice_loss: 0.79646928, train_dice: 0.58309846
valid_loss: 0.58723776, valid_bce_loss: 0.34270482, valid_dice_loss: 0.83177070, valid_dice: 0.43882573

Epoch: 36/200 |  time : 09/06/19-08:36:08
=================================================================
train_loss: 0.54548739, train_bce_loss: 0.29376665, train_dice_loss: 0.79720814, train_dice: 0.58197360
valid_loss: 0.58613367, valid_bce_loss: 0.35020972, valid_dice_loss: 0.82205763, valid_dice: 0.42612738

Epoch: 37/200 |  time : 09/06/19-08:54:09
=================================================================
train_loss: 0.54293449, train_bce_loss: 0.29105599, train_dice_loss: 0.79481300, train_dice: 0.58665978
valid_loss: 0.58796336, valid_bce_loss: 0.34779406, valid_dice_loss: 0.82813265, valid_dice: 0.41637877

Epoch: 38/200 |  time : 09/06/19-09:12:08
=================================================================
train_loss: 0.54328330, train_bce_loss: 0.29156526, train_dice_loss: 0.79500133, train_dice: 0.58670169
valid_loss: 0.58891901, valid_bce_loss: 0.34796103, valid_dice_loss: 0.82987698, valid_dice: 0.41843556

Epoch: 39/200 |  time : 09/06/19-09:30:08
=================================================================
train_loss: 0.54200166, train_bce_loss: 0.29003357, train_dice_loss: 0.79396975, train_dice: 0.59969270
valid_loss: 0.58789743, valid_bce_loss: 0.35221771, valid_dice_loss: 0.82357715, valid_dice: 0.42751086

Epoch: 40/200 |  time : 09/06/19-09:48:07
=================================================================
train_loss: 0.54118084, train_bce_loss: 0.28964333, train_dice_loss: 0.79271836, train_dice: 0.59310900
valid_loss: 0.58654037, valid_bce_loss: 0.34843726, valid_dice_loss: 0.82464348, valid_dice: 0.42190837

Epoch: 41/200 |  time : 09/06/19-10:06:06
=================================================================
