Train on 1110 mini-batches, validate on 277 mini-batches
Epoch: 1/200 |  time : 09/05/19-22:25:49
=================================================================
train_loss: 0.64555117, train_bce_loss: 0.42583591, train_dice_loss: 0.86526643, train_dice: 0.32591668
valid_loss: 0.59327550, valid_bce_loss: 0.36036515, valid_dice_loss: 0.82618584, valid_dice: 0.39577585
******** Validation loss improved from inf to 0.5932754966756497, saving state ********

Epoch: 2/200 |  time : 09/05/19-22:43:51
=================================================================
train_loss: 0.63486944, train_bce_loss: 0.41188679, train_dice_loss: 0.85785210, train_dice: 0.28258425
valid_loss: 0.58309036, valid_bce_loss: 0.34758405, valid_dice_loss: 0.81859666, valid_dice: 0.38450488
******** Validation loss improved from 0.5932754966756497 to 0.583090356325845, saving state ********

Epoch: 3/200 |  time : 09/05/19-23:01:42
=================================================================
train_loss: 0.62844672, train_bce_loss: 0.40231028, train_dice_loss: 0.85458317, train_dice: 0.32192383
valid_loss: 0.58307926, valid_bce_loss: 0.34204436, valid_dice_loss: 0.82411416, valid_dice: 0.41997801
******** Validation loss improved from 0.583090356325845 to 0.5830792586725971, saving state ********

Epoch: 4/200 |  time : 09/05/19-23:19:35
=================================================================
train_loss: 0.62505373, train_bce_loss: 0.39772972, train_dice_loss: 0.85237774, train_dice: 0.34539834
valid_loss: 0.56949491, valid_bce_loss: 0.32908262, valid_dice_loss: 0.80990719, valid_dice: 0.44504006
******** Validation loss improved from 0.5830792586725971 to 0.5694949086822758, saving state ********

Epoch: 5/200 |  time : 09/05/19-23:37:25
=================================================================
train_loss: 0.62303011, train_bce_loss: 0.39517686, train_dice_loss: 0.85088336, train_dice: 0.35992620
valid_loss: 0.57379981, valid_bce_loss: 0.33395655, valid_dice_loss: 0.81364307, valid_dice: 0.46911410

Epoch: 6/200 |  time : 09/05/19-23:55:18
=================================================================
train_loss: 0.61934514, train_bce_loss: 0.38988617, train_dice_loss: 0.84880411, train_dice: 0.37923022
valid_loss: 0.57110251, valid_bce_loss: 0.33134265, valid_dice_loss: 0.81086237, valid_dice: 0.46619786

Epoch: 7/200 |  time : 09/06/19-00:13:09
=================================================================
train_loss: 0.61641608, train_bce_loss: 0.38570300, train_dice_loss: 0.84712915, train_dice: 0.39592077
valid_loss: 0.57987199, valid_bce_loss: 0.34237699, valid_dice_loss: 0.81736699, valid_dice: 0.45295472

Epoch: 8/200 |  time : 09/06/19-00:30:57
=================================================================
train_loss: 0.61359773, train_bce_loss: 0.38136380, train_dice_loss: 0.84583166, train_dice: 0.40646677
valid_loss: 0.58378843, valid_bce_loss: 0.33851429, valid_dice_loss: 0.82906258, valid_dice: 0.49691404

Epoch: 9/200 |  time : 09/06/19-00:48:46
=================================================================
train_loss: 0.61049146, train_bce_loss: 0.37653479, train_dice_loss: 0.84444812, train_dice: 0.43175704
valid_loss: 0.57517810, valid_bce_loss: 0.33209917, valid_dice_loss: 0.81825703, valid_dice: 0.49584519

Epoch: 10/200 |  time : 09/06/19-01:06:38
=================================================================
train_loss: 0.60717740, train_bce_loss: 0.37142950, train_dice_loss: 0.84292530, train_dice: 0.43802504
valid_loss: 0.58090983, valid_bce_loss: 0.33969204, valid_dice_loss: 0.82212762, valid_dice: 0.48947252
Epoch     9: reducing learning rate of group 0 to 5.0000e-03.

Epoch: 11/200 |  time : 09/06/19-01:24:28
=================================================================
train_loss: 0.59836283, train_bce_loss: 0.35879333, train_dice_loss: 0.83793233, train_dice: 0.46898949
valid_loss: 0.57757370, valid_bce_loss: 0.33496860, valid_dice_loss: 0.82017881, valid_dice: 0.49122341

Epoch: 12/200 |  time : 09/06/19-01:42:16
=================================================================
train_loss: 0.59437975, train_bce_loss: 0.35390894, train_dice_loss: 0.83485055, train_dice: 0.47070217
valid_loss: 0.57145625, valid_bce_loss: 0.32938747, valid_dice_loss: 0.81352504, valid_dice: 0.47886218

Epoch: 13/200 |  time : 09/06/19-02:00:07
=================================================================
train_loss: 0.59074103, train_bce_loss: 0.34883342, train_dice_loss: 0.83264864, train_dice: 0.48230362
valid_loss: 0.58411376, valid_bce_loss: 0.34017922, valid_dice_loss: 0.82804830, valid_dice: 0.48162660

Epoch: 14/200 |  time : 09/06/19-02:17:58
=================================================================
train_loss: 0.58934392, train_bce_loss: 0.34697431, train_dice_loss: 0.83171353, train_dice: 0.48140319
valid_loss: 0.57793368, valid_bce_loss: 0.33187020, valid_dice_loss: 0.82399715, valid_dice: 0.49507892

Epoch: 15/200 |  time : 09/06/19-02:35:47
=================================================================
train_loss: 0.58667614, train_bce_loss: 0.34337049, train_dice_loss: 0.82998178, train_dice: 0.47939297
valid_loss: 0.58048516, valid_bce_loss: 0.33691161, valid_dice_loss: 0.82405870, valid_dice: 0.47617415

Epoch: 16/200 |  time : 09/06/19-02:53:39
=================================================================
train_loss: 0.58412927, train_bce_loss: 0.33994664, train_dice_loss: 0.82831190, train_dice: 0.48295128
valid_loss: 0.58180492, valid_bce_loss: 0.33769291, valid_dice_loss: 0.82591693, valid_dice: 0.46260483
Epoch    15: reducing learning rate of group 0 to 2.5000e-03.

Epoch: 17/200 |  time : 09/06/19-03:11:28
=================================================================
train_loss: 0.57752236, train_bce_loss: 0.33101402, train_dice_loss: 0.82403071, train_dice: 0.50129544
valid_loss: 0.57709797, valid_bce_loss: 0.33524900, valid_dice_loss: 0.81894694, valid_dice: 0.44680391

Epoch: 18/200 |  time : 09/06/19-03:29:20
=================================================================
train_loss: 0.57442008, train_bce_loss: 0.32677554, train_dice_loss: 0.82206462, train_dice: 0.50883918
valid_loss: 0.58229369, valid_bce_loss: 0.34210747, valid_dice_loss: 0.82247992, valid_dice: 0.43745808

Epoch: 19/200 |  time : 09/06/19-03:47:08
=================================================================
train_loss: 0.57109357, train_bce_loss: 0.32331332, train_dice_loss: 0.81887383, train_dice: 0.51303949
valid_loss: 0.59118160, valid_bce_loss: 0.34586963, valid_dice_loss: 0.83649357, valid_dice: 0.44423288

Epoch: 20/200 |  time : 09/06/19-04:05:01
=================================================================
train_loss: 0.56989982, train_bce_loss: 0.32179713, train_dice_loss: 0.81800250, train_dice: 0.51349779
valid_loss: 0.57996130, valid_bce_loss: 0.33585515, valid_dice_loss: 0.82406746, valid_dice: 0.44015969

Epoch: 21/200 |  time : 09/06/19-04:22:54
=================================================================
train_loss: 0.56663682, train_bce_loss: 0.31815932, train_dice_loss: 0.81511431, train_dice: 0.51939495
valid_loss: 0.57906916, valid_bce_loss: 0.33632695, valid_dice_loss: 0.82181139, valid_dice: 0.43204833

Epoch: 22/200 |  time : 09/06/19-04:40:43
=================================================================
train_loss: 0.56549087, train_bce_loss: 0.31662743, train_dice_loss: 0.81435431, train_dice: 0.52194654
valid_loss: 0.57799545, valid_bce_loss: 0.33954752, valid_dice_loss: 0.81644337, valid_dice: 0.42480713
Epoch    21: reducing learning rate of group 0 to 1.2500e-03.

Epoch: 23/200 |  time : 09/06/19-04:58:34
=================================================================
train_loss: 0.56074659, train_bce_loss: 0.31078145, train_dice_loss: 0.81071173, train_dice: 0.53840989
valid_loss: 0.58150772, valid_bce_loss: 0.33574727, valid_dice_loss: 0.82726817, valid_dice: 0.40955390

Epoch: 24/200 |  time : 09/06/19-05:16:23
=================================================================
train_loss: 0.55698877, train_bce_loss: 0.30652827, train_dice_loss: 0.80744926, train_dice: 0.54042315
valid_loss: 0.57785375, valid_bce_loss: 0.33655645, valid_dice_loss: 0.81915104, valid_dice: 0.41787730

Epoch: 25/200 |  time : 09/06/19-05:34:13
=================================================================
train_loss: 0.55664493, train_bce_loss: 0.30560116, train_dice_loss: 0.80768871, train_dice: 0.54648176
valid_loss: 0.58331271, valid_bce_loss: 0.34232999, valid_dice_loss: 0.82429544, valid_dice: 0.40981623

Epoch: 26/200 |  time : 09/06/19-05:52:03
=================================================================
train_loss: 0.55551541, train_bce_loss: 0.30468264, train_dice_loss: 0.80634819, train_dice: 0.55076582
valid_loss: 0.58462235, valid_bce_loss: 0.34225546, valid_dice_loss: 0.82698924, valid_dice: 0.41056737

Epoch: 27/200 |  time : 09/06/19-06:09:53
=================================================================
train_loss: 0.55288835, train_bce_loss: 0.30229892, train_dice_loss: 0.80347778, train_dice: 0.55541160
valid_loss: 0.58113196, valid_bce_loss: 0.33689307, valid_dice_loss: 0.82537085, valid_dice: 0.42193264

Epoch: 28/200 |  time : 09/06/19-06:27:44
=================================================================
train_loss: 0.55165124, train_bce_loss: 0.30048605, train_dice_loss: 0.80281644, train_dice: 0.55652137
valid_loss: 0.58720714, valid_bce_loss: 0.34380411, valid_dice_loss: 0.83061017, valid_dice: 0.40452154
Epoch    27: reducing learning rate of group 0 to 1.0000e-03.

Epoch: 29/200 |  time : 09/06/19-06:45:34
=================================================================
train_loss: 0.54986719, train_bce_loss: 0.29853239, train_dice_loss: 0.80120198, train_dice: 0.55990267
valid_loss: 0.58228864, valid_bce_loss: 0.34727136, valid_dice_loss: 0.81730593, valid_dice: 0.38863263

Epoch: 30/200 |  time : 09/06/19-07:03:25
=================================================================
train_loss: 0.54878088, train_bce_loss: 0.29712755, train_dice_loss: 0.80043422, train_dice: 0.56932771
valid_loss: 0.58419165, valid_bce_loss: 0.34096923, valid_dice_loss: 0.82741405, valid_dice: 0.38873088

Epoch: 31/200 |  time : 09/06/19-07:21:14
=================================================================
train_loss: 0.54817024, train_bce_loss: 0.29659027, train_dice_loss: 0.79975020, train_dice: 0.57004022
valid_loss: 0.58618364, valid_bce_loss: 0.34602360, valid_dice_loss: 0.82634368, valid_dice: 0.39454306

Epoch: 32/200 |  time : 09/06/19-07:39:06
=================================================================
train_loss: 0.54521589, train_bce_loss: 0.29391535, train_dice_loss: 0.79651643, train_dice: 0.57384714
valid_loss: 0.58295106, valid_bce_loss: 0.34698606, valid_dice_loss: 0.81891605, valid_dice: 0.39665258

Epoch: 33/200 |  time : 09/06/19-07:56:56
=================================================================
train_loss: 0.54576877, train_bce_loss: 0.29396416, train_dice_loss: 0.79757338, train_dice: 0.57724716
valid_loss: 0.58350863, valid_bce_loss: 0.34636163, valid_dice_loss: 0.82065562, valid_dice: 0.39478294

Epoch: 34/200 |  time : 09/06/19-08:14:45
=================================================================
train_loss: 0.54404833, train_bce_loss: 0.29225467, train_dice_loss: 0.79584199, train_dice: 0.58368104
valid_loss: 0.58747845, valid_bce_loss: 0.35315083, valid_dice_loss: 0.82180607, valid_dice: 0.37225973

Epoch: 35/200 |  time : 09/06/19-08:32:36
=================================================================
train_loss: 0.54310770, train_bce_loss: 0.29183842, train_dice_loss: 0.79437698, train_dice: 0.58298187
valid_loss: 0.58683699, valid_bce_loss: 0.34974676, valid_dice_loss: 0.82392723, valid_dice: 0.38446126

Epoch: 36/200 |  time : 09/06/19-08:50:26
=================================================================
train_loss: 0.54318251, train_bce_loss: 0.29150684, train_dice_loss: 0.79485819, train_dice: 0.58530568
valid_loss: 0.58551072, valid_bce_loss: 0.34694834, valid_dice_loss: 0.82407309, valid_dice: 0.38276195

Epoch: 37/200 |  time : 09/06/19-09:08:18
=================================================================
train_loss: 0.54193287, train_bce_loss: 0.29042135, train_dice_loss: 0.79344440, train_dice: 0.59206159
valid_loss: 0.58992795, valid_bce_loss: 0.34920797, valid_dice_loss: 0.83064795, valid_dice: 0.39440285

Epoch: 38/200 |  time : 09/06/19-09:26:08
=================================================================
train_loss: 0.54117429, train_bce_loss: 0.28940870, train_dice_loss: 0.79293989, train_dice: 0.59379991
valid_loss: 0.58805445, valid_bce_loss: 0.35016406, valid_dice_loss: 0.82594483, valid_dice: 0.38135051

Epoch: 39/200 |  time : 09/06/19-09:43:59
=================================================================
train_loss: 0.53956439, train_bce_loss: 0.28776880, train_dice_loss: 0.79135999, train_dice: 0.59661657
valid_loss: 0.58805976, valid_bce_loss: 0.35043487, valid_dice_loss: 0.82568464, valid_dice: 0.39752649

Epoch: 40/200 |  time : 09/06/19-10:01:50
=================================================================
train_loss: 0.53912031, train_bce_loss: 0.28748360, train_dice_loss: 0.79075703, train_dice: 0.59897857
valid_loss: 0.58846626, valid_bce_loss: 0.35194997, valid_dice_loss: 0.82498255, valid_dice: 0.37610677

Epoch: 41/200 |  time : 09/06/19-10:19:41
=================================================================
